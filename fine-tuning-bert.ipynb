{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer\n",
    "import hazm\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text normalization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = hazm.Normalizer()\n",
    "def text_normalization(text):\n",
    "    text = normalizer.normalize(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id   = \"sbunlp/fabert\"\n",
    "model_name = \"FaBERT\"\n",
    "cache_dir  = f\"./Model-FaBERT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at sbunlp/fabert and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "                                            model_id,\n",
    "                                            cache_dir=cache_dir,\n",
    "                                        )\n",
    "\n",
    "# Load the model\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "                                            model_id,\n",
    "                                            cache_dir=cache_dir,\n",
    "                                            # device_map=device,\n",
    "                                            num_labels=2\n",
    "                                        )\n",
    "\n",
    "# model.to(device)  # Move the model to the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 124,442,882\n",
      "Trainable parameters: 124,442,882\n"
     ]
    }
   ],
   "source": [
    "def print_model_parameters(model):\n",
    "    # Total number of parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    # Trainable parameters\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# Example usage\n",
    "print_model_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSON data\n",
    "def load_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/media/amin/T7 SHIELD/Projects/Persian-Informal-Formal/Datasets/ParsMap\"\n",
    "\n",
    "train_data = load_data(f\"{base_path}/ParsMap-train.json\")\n",
    "val_data   = load_data(f\"{base_path}/ParsMap-val.json\")\n",
    "test_data  = load_data(f\"{base_path}/ParsMap-test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#of training samples: 35009\n",
      "\n",
      "\n",
      "#of validation samples: 7502\n",
      "\n",
      "\n",
      "#of testing samples: 7503\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'#of training samples: {len(train_data.items())}')\n",
    "print('\\n')\n",
    "print(f'#of validation samples: {len(val_data.items())}')\n",
    "print('\\n')\n",
    "print(f'#of testing samples: {len(test_data.items())}')\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data into lists for binary classification\n",
    "def prepare_data(data):\n",
    "    inputs, labels = [], []\n",
    "    for _, value in tqdm(data.items()):\n",
    "        try:\n",
    "            informal_text = value['inFormalForm']\n",
    "            informal_text = text_normalization(informal_text)\n",
    "            inputs.append(informal_text)\n",
    "            labels.append(0)  # Add 0 for inFormalForm\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "        try:\n",
    "            formal_text  = value['formalForm']\n",
    "            formal_text = text_normalization(formal_text)\n",
    "            inputs.append(formal_text)\n",
    "            labels.append(1)  # Add 1 for formalForm\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "\n",
    "    return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 15800/35009 [00:02<00:03, 6166.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: 'float' object has no attribute 'translate'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35009/35009 [00:05<00:00, 6045.99it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7502/7502 [00:01<00:00, 6045.56it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7503/7503 [00:01<00:00, 6113.16it/s]\n"
     ]
    }
   ],
   "source": [
    "train_texts, train_labels = prepare_data(train_data)\n",
    "val_texts,   val_labels   = prepare_data(val_data)\n",
    "test_texts,  test_labels  = prepare_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the data\n",
    "\n",
    "def tokenize_data(texts, labels):\n",
    "    tokenized = tokenizer(\n",
    "        texts, padding=True, truncation=True, max_length=512, return_tensors=\"pt\"\n",
    "    )\n",
    "    tokenized[\"labels\"] = torch.tensor(labels)\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenize_data(train_texts, train_labels)\n",
    "val_encodings   = tokenize_data(val_texts, val_labels)\n",
    "test_encodings  = tokenize_data(test_texts, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encodings.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: tensor[idx] for key, tensor in self.encodings.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TextDataset(train_encodings)\n",
    "val_dataset   = TextDataset(val_encodings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amin/anaconda3/envs/torch/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",  # Evaluate at the end of each epoch\n",
    "    save_strategy=\"epoch\",        # Save at the end of each epoch\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    logging_dir=\"./logs\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05dc76b20b8f4a59832fc7e3e520890a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6567 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0763, 'grad_norm': 0.01840325817465782, 'learning_rate': 4.6193086645347957e-05, 'epoch': 0.23}\n",
      "{'loss': 0.0359, 'grad_norm': 0.12912239134311676, 'learning_rate': 4.238617329069591e-05, 'epoch': 0.46}\n",
      "{'loss': 0.0357, 'grad_norm': 4.717024803161621, 'learning_rate': 3.857925993604386e-05, 'epoch': 0.69}\n",
      "{'loss': 0.0233, 'grad_norm': 17.164199829101562, 'learning_rate': 3.4772346581391806e-05, 'epoch': 0.91}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "832fd01f1e2a4081bb8aab309bcce76a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/469 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.022439230233430862, 'eval_runtime': 27.9329, 'eval_samples_per_second': 537.144, 'eval_steps_per_second': 16.79, 'epoch': 1.0}\n",
      "{'loss': 0.0188, 'grad_norm': 0.044183868914842606, 'learning_rate': 3.096543322673976e-05, 'epoch': 1.14}\n",
      "{'loss': 0.0094, 'grad_norm': 0.005505237728357315, 'learning_rate': 2.715851987208771e-05, 'epoch': 1.37}\n",
      "{'loss': 0.008, 'grad_norm': 0.0010621192632243037, 'learning_rate': 2.3351606517435665e-05, 'epoch': 1.6}\n",
      "{'loss': 0.0132, 'grad_norm': 0.002697585616260767, 'learning_rate': 1.954469316278362e-05, 'epoch': 1.83}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c5311bc35154c40bbcfcd29842d5ea5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/469 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.021635230630636215, 'eval_runtime': 27.9863, 'eval_samples_per_second': 536.119, 'eval_steps_per_second': 16.758, 'epoch': 2.0}\n",
      "{'loss': 0.0102, 'grad_norm': 0.048293791711330414, 'learning_rate': 1.5737779808131566e-05, 'epoch': 2.06}\n",
      "{'loss': 0.0032, 'grad_norm': 0.0008392877643927932, 'learning_rate': 1.1930866453479519e-05, 'epoch': 2.28}\n",
      "{'loss': 0.0026, 'grad_norm': 0.04795552045106888, 'learning_rate': 8.123953098827471e-06, 'epoch': 2.51}\n",
      "{'loss': 0.0033, 'grad_norm': 0.0014071812620386481, 'learning_rate': 4.317039744175423e-06, 'epoch': 2.74}\n",
      "{'loss': 0.0038, 'grad_norm': 0.0016569598810747266, 'learning_rate': 5.101263895233745e-07, 'epoch': 2.97}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b674f772b55042f29e36c88313ffc847",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/469 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.022546423599123955, 'eval_runtime': 27.9352, 'eval_samples_per_second': 537.1, 'eval_steps_per_second': 16.789, 'epoch': 3.0}\n",
      "{'train_runtime': 2036.2132, 'train_samples_per_second': 103.158, 'train_steps_per_second': 3.225, 'train_loss': 0.018555654765215795, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6567, training_loss=0.018555654765215795, metrics={'train_runtime': 2036.2132, 'train_samples_per_second': 103.158, 'train_steps_per_second': 3.225, 'total_flos': 1.0794285212778e+16, 'train_loss': 0.018555654765215795, 'epoch': 3.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset for testing\n",
    "test_dataset = TextDataset(test_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfc00db351c84e9390602a2ed13cf722",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/469 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Predict\n",
    "predictions = trainer.predict(test_dataset)\n",
    "preds = torch.argmax(torch.tensor(predictions.predictions), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Mismatch       1.00      0.99      1.00      7503\n",
      "       Match       0.99      1.00      1.00      7503\n",
      "\n",
      "    accuracy                           1.00     15006\n",
      "   macro avg       1.00      1.00      1.00     15006\n",
      "weighted avg       1.00      1.00      1.00     15006\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate classification report\n",
    "print(classification_report(test_labels, preds.numpy(), target_names=[\"Mismatch\", \"Match\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./fine_tuned_fabert/tokenizer_config.json',\n",
       " './fine_tuned_fabert/special_tokens_map.json',\n",
       " './fine_tuned_fabert/vocab.txt',\n",
       " './fine_tuned_fabert/added_tokens.json')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./fine_tuned_fabert\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_fabert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader for training\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(3):  # Number of epochs\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        batch = {key: val.to(device) for key, val in batch.items()}  # Move batch to GPU\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch = {key: val.to(device) for key, val in batch.items()}  # Move batch to GPU\n",
    "            outputs = model(**batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Evaluation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 938/938 [00:29<00:00, 31.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Mismatch       1.00      0.99      1.00      7503\n",
      "       Match       0.99      1.00      1.00      7503\n",
      "\n",
      "    accuracy                           1.00     15006\n",
      "   macro avg       1.00      1.00      1.00     15006\n",
      "weighted avg       1.00      1.00      1.00     15006\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "model.eval()\n",
    "predictions, true_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader):\n",
    "        batch = {key: val.to(device) for key, val in batch.items()}  # Move batch to GPU\n",
    "        outputs = model(**batch)\n",
    "        logits = outputs.logits\n",
    "        predictions.extend(torch.argmax(logits, dim=1).cpu().numpy())\n",
    "        true_labels.extend(batch[\"labels\"].cpu().numpy())\n",
    "\n",
    "# Calculate metrics\n",
    "print(classification_report(true_labels, predictions, target_names=[\"Mismatch\", \"Match\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
